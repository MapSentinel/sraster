---
title: "Tutorial of sraster"
author: "DGT Portugal, William Martinez"
date: "05/08/2019"
fig_caption: TRUE
output:
  html_document: 
    theme: journal
    toc: true
    toc_depth: 4
    number_section: true
    code_folding: hide
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
```

# Introduction


I want to select 1000 samples per class. To do so also I will consider to keep the same amount of samples per polygons, or at least the same proportion.

# Importing data

```{r}

stat_metrics = NULL
overall_accuracy = NULL


for(iter in 1:2){
  
file_directory = 'C:\\IPSTERS\\sraster\\ins3\\sampling_3'
list_files = lapply(list.files(file_directory),function(x){paste0(file_directory,"\\",x)})

group_by = 'Object'
nsamples = 1000

sampling_strata <- function(file_x, nsamples, group_by ){
  x = read.csv(file_x,sep = ",", header = TRUE)
  n_total_samples = nrow(x)
  
  samples_strata = function(y,n_total_samples,nsamples){
    if(n_total_samples < nsamples)
    {
      nsamples = n_total_samples
      message("we have less samples that those expected")
    }
    n_rows = nrow(y)
    perc_samples = n_rows/n_total_samples
    n_samples_class = round(nsamples * perc_samples)
    random_index = sample(1:n_rows,size = n_samples_class,replace = FALSE)
    return(y[random_index,])
    }
  
  #split
  x_split = split(x,x[,group_by])
  x_split_random = lapply(x_split, samples_strata, n_total_samples, nsamples)
  x_random = do.call("rbind", x_split_random)
  return(x_random)
  cat(file_x)
}

data_random_split = lapply(list_files, sampling_strata, nsamples, group_by)


#===============================
#classification random forest
#===============================
#Since we have 22 polygons per class, I will select 15 for training an 7 for testing. 
function_train_selection <- function(x){
  polygons_class = unique(x$Object)
  ind = sample(2, length(polygons_class), replace = TRUE, prob = c(0.7,0.3))
  train_class = polygons_class[ind==1]
  test_class = polygons_class[ind==2]
  train_df = x[x$Object %in% train_class,]
  train_df$type = "Training"
  test_df = x[x$Object %in% test_class,]
  test_df$type = "Test"
  df_result = rbind(train_df,test_df)
  return(df_result)
}

data_random_split2 = lapply(data_random_split, function_train_selection)

data_random = do.call("rbind",data_random_split2)
#write.csv(data_random,"output5.csv")


#===============================
#Removing nans
#===============================

remove_na_df = function(x){
  x$row <- 1:nrow(x)
  list_rows = split(x,x$row)
  list_rows_wn = lapply(list_rows,function(r){if(all(!is.na(r))){return(r)}})
  result_df = do.call("rbind", list_rows_wn)
  result_df$row <- NULL
  return(result_df)
}

data_random2 = remove_na_df(data_random)

#============================
#adding more feaatures
#============================
names_bands = colnames(data_random2)
ind_NDVI = grep("NDVI",names_bands)
ind_NDBI = grep("NDBI",names_bands)
ind_NDMIR = grep("NDMIR",names_bands)
indeces = list(data_random2[,ind_NDVI],data_random2[,ind_NDBI],data_random2[,ind_NDMIR])
indeces_names = c("NDVI","NDBI","NDMIR")

statistic_mean = function(y, name_y) mean(y, na.rm=TRUE)
statistic_min = function(y) min(y, na.rm=TRUE)
statistic_max = function(y) max(y, na.rm=TRUE)
statistic_var = function(y) var(y, na.rm=TRUE)
statistic_q10 = function(y) quantile(y, probs = c(0.1), na.rm=TRUE)
statistic_q25 = function(y) quantile(y, probs = c(0.25), na.rm=TRUE)
statistic_q50 = function(y) quantile(y, probs = c(0.50), na.rm=TRUE)
statistic_q75 = function(y) quantile(y, probs = c(0.75), na.rm=TRUE)
statistic_q90 = function(y) quantile(y, probs = c(0.90), na.rm=TRUE)

statistic = list(statistic_mean, statistic_min, statistic_max, statistic_var, statistic_q10, statistic_q25, statistic_q50, statistic_q75, statistic_q90)
statistic_names = c("mean","min","max","var","q10","q25","q50","q75","q90")
output = NULL
for(j in 1:length(statistic)){
  statistic_w = lapply(indeces, function(x) apply(x,1,statistic[[j]]))
  df_stat = do.call("cbind",statistic_w)
  colnames(df_stat)<- paste0(indeces_names, statistic_names[j])
  output = cbind(output,df_stat)
}
output_df = as.data.frame(output)
#============================

data_random3 = cbind(data_random2, output_df)

train = data_random3[data_random3$type == "Training",-c(1,2,3,96)]
test = data_random3[data_random3$type == "Test",-c(1,2,3,96)]

#===============================
#Modelling
#===============================
#set.seed(222)
model_rf = randomForest::randomForest(Label~. , data = train,ntree = 500)
print(model_rf)

#===============================
#Prediction
#===============================

library(caret)
pred_test = predict(model_rf, test)
result_test = caret::confusionMatrix(pred_test, test$Label)

#===============================
#Accuracies
#===============================

stat_metrics = rbind(stat_metrics, result_test$byClass[,c(1,3,7)])
overall_accuracy = rbind(overall_accuracy,result_test$overall)


cat("ready",iter)
}

```


```{r}
#gc()
#rm(list=ls())

#write.csv(stat_metrics,"S3_stat_metrics_3000s_ndvi.csv")
#write.csv(overall_accuracy ,"S3_overall_accuracy_3000s_ndvi.csv")
```
